# Universal File Format Converter - Enhanced

A comprehensive PyQt6-based GUI application for converting between multiple file formats commonly used in machine learning and data science.

## üöÄ Features

### Multi-Format Support
- **Input/Output Formats**: CSV, TSV, TXT, Parquet, Excel (XLSX/XLS), JSON, HDF5, Pickle, NumPy, MATLAB, Feather, Arrow, SQLite, Joblib
- **Automatic Format Detection**: Detects file formats based on extension and content
- **Bulk Conversion**: Convert multiple files in a single operation
- **Flexible Output**: Combine into single file or keep as multiple files

### Advanced Column Management
- **Column Selection**: Advanced column selection with search and filtering
- **Save/Load Lists**: Save and load column selection lists for reuse
- **Large Dataset Support**: Efficient handling of files with 2000+ columns
- **Memory Optimization**: Chunked processing for large datasets

### File Splitting & Management
- **Multiple Splitting Methods**: Split by row count, file size, time intervals, or custom conditions
- **Large File Handling**: Break down extremely large datasets into manageable chunks
- **Flexible Output**: Customizable filename patterns and output directories
- **Compression Options**: Optimize file sizes with various compression algorithms
- **Progress Tracking**: Real-time progress updates during splitting operations

### User Experience
- **Modern Interface**: Clean, intuitive PyQt6-based user interface
- **Progress Tracking**: Real-time progress updates and detailed logging
- **Error Handling**: Robust error handling with detailed error reporting
- **Cancel Support**: Ability to cancel long-running conversions
- **Log Management**: Save conversion logs for future reference

## üìã Supported Formats

### Data Formats
| Format | Extensions | Description |
|--------|------------|-------------|
| CSV | `.csv` | Comma-separated values |
| TSV | `.tsv`, `.txt` | Tab-separated values |
| Parquet | `.parquet`, `.pq` | Columnar storage format |
| Excel | `.xlsx`, `.xls` | Microsoft Excel format |
| JSON | `.json` | JavaScript Object Notation |
| HDF5 | `.h5`, `.hdf5` | Hierarchical Data Format 5 |
| Pickle | `.pkl`, `.pickle` | Python pickle format |
| NumPy | `.npy`, `.npz` | NumPy array format |
| MATLAB | `.mat` | MATLAB data format |
| Feather | `.feather` | Fast columnar data format |
| Arrow | `.arrow` | Apache Arrow format |
| SQLite | `.db`, `.sqlite`, `.sqlite3` | SQLite database |
| Joblib | `.joblib` | Joblib serialization |

## üõ†Ô∏è Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Install Dependencies
```bash
# Install enhanced requirements
pip install -r requirements_enhanced.txt

# Or install core dependencies only
pip install PyQt6 pandas pyarrow numpy openpyxl h5py scipy joblib
```

## üöÄ Usage

### Launch the Application
```bash
python File_Convert_Compile_Enhanced.py
```

### Basic Workflow
1. **Select Input Files**: Use "Browse Folder" or "Browse Files" to select input files
2. **Configure Output**: Choose output format and path
3. **Select Columns** (Optional): Choose specific columns to include
4. **Start Conversion**: Click "Start Conversion" to begin

### Advanced Features
- **Column Selection**: Uncheck "Use all columns" to select specific columns
- **Bulk Processing**: Select multiple files or entire folders
- **Format Options**: Different formats support various options (see format-specific documentation)
- **Logging**: Monitor conversion progress and save logs

### File Splitting Features
- **Row-based Splitting**: Split files into chunks of specified row counts
- **Size-based Splitting**: Split files to achieve target file sizes
- **Time-based Splitting**: Split time-series data by time intervals
- **Custom Splitting**: Use Python expressions for complex splitting logic
- **Output Management**: Customize filename patterns and output directories

#### Splitting Examples:
```python
# Row-based: Split into 50,000 row chunks
rows_per_file = 50000

# Size-based: Split into 100MB files
max_file_size_mb = 100.0

# Time-based: Split by daily intervals
time_column = "timestamp"
interval_hours = 24.0

# Custom: Split when category changes
custom_condition = "row['category'] != previous_row['category']"
```

### Configuration

### Format-Specific Options
Each format supports different options that can be configured:

- **CSV/TSV**: Delimiter, encoding, compression
- **Parquet**: Compression, row group size
- **Excel**: Sheet selection, formatting
- **HDF5**: Compression, chunk size
- **JSON**: Orientation, date format

### Performance Tuning
- **Chunk Size**: Adjust for memory usage vs. speed
- **Combine Files**: Enable for single output, disable for multiple files
- **Column Selection**: Reduce memory usage by selecting only needed columns

### File Splitting Performance
- **Memory Efficiency**: Splitting reduces peak memory usage
- **Parallel Processing**: Split files can be processed in parallel
- **Storage Optimization**: Choose appropriate compression for your use case
- **Network Transfer**: Smaller files transfer faster over networks

### Benchmarks
- **CSV to Parquet**: ~2-5x faster than pandas alone
- **Large Files**: Handles files with millions of rows efficiently
- **Memory Usage**: 50-80% reduction compared to loading entire files

## üìä Performance

### Large Dataset Optimization
- **Memory Efficient**: Processes files in chunks to minimize memory usage
- **Parallel Processing**: Multi-threaded conversion for better performance
- **Format Optimization**: Uses optimal libraries for each format (PyArrow for Parquet, etc.)

### Benchmarks
- **CSV to Parquet**: ~2-5x faster than pandas alone
- **Large Files**: Handles files with millions of rows efficiently
- **Memory Usage**: 50-80% reduction compared to loading entire files

## üêõ Troubleshooting

### Common Issues

**Format Detection Fails**
- Check file extension matches supported formats
- Verify file is not corrupted
- Try manual format selection

**Memory Errors**
- Reduce chunk size in settings
- Select fewer columns
- Process files individually instead of combining

**Conversion Errors**
- Check file format compatibility
- Verify all files have consistent structure
- Review conversion log for specific errors

### Log Files
- Application logs: `file_converter.log`
- Conversion logs: Can be saved from GUI
- Debug information: Check console output

## üîÑ Version History

### Enhanced v1.1 (with File Splitting)
- **File Splitting**: Multiple methods for breaking large datasets
- **Advanced UI**: Comprehensive splitting configuration dialog
- **Performance**: Optimized memory usage for large files
- **Flexibility**: Custom splitting conditions and output patterns

### Enhanced v1.0
- Multi-format support (13+ formats)
- Automatic format detection
- Enhanced column selection
- Improved error handling
- Better performance optimization

### Previous Versions
- **File_Convert_Compile_r0**: Original CSV to Parquet converter
- **csv_to_parquet_converter_enhanced**: Enhanced CSV converter

## ü§ù Contributing

### Development Setup
```bash
# Clone repository
git clone <repository-url>
cd csv_converter

# Install development dependencies
pip install -r requirements_enhanced.txt

# Run tests
pytest tests/

# Launch application
python File_Convert_Compile_Enhanced.py
```

### Adding New Formats
1. Add format definition to `SUPPORTED_FORMATS`
2. Implement read/write methods in `DataReader`/`DataWriter`
3. Add format detection logic to `FileFormatDetector`
4. Update UI components as needed

## üìÑ License

This project is provided as-is for educational and development purposes.

## üÜò Support

For issues or questions:
1. Check the troubleshooting section
2. Review the conversion logs
3. Ensure all dependencies are properly installed
4. Verify file formats and permissions

## üîÆ Future Enhancements

### Planned Features
- **TFRecord Support**: TensorFlow record format
- **ONNX Support**: Open Neural Network Exchange
- **PMML Support**: Predictive Model Markup Language
- **Cloud Storage**: Direct upload to cloud services
- **Batch Processing**: Scheduled conversion jobs
- **API Interface**: REST API for programmatic access

### Performance Improvements
- **GPU Acceleration**: CUDA support for large datasets
- **Distributed Processing**: Multi-machine conversion
- **Streaming**: Real-time data conversion
- **Caching**: Intelligent format caching 